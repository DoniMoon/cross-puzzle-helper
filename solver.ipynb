{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b0eb92-abe6-467b-96d6-d062fd26e9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "all_word_list = json.load(open('word_freq_sorted.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3cb67-5653-4681-b098-f1f843bcaa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "bert_cache_dir = 'bert_cache_dir.pt'\n",
    "bert_cache_dict = torch.load(bert_cache_dir) if os.path.exists(bert_cache_dir) else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e28da1-e983-4d8c-8696-bb7a9f6ee194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model,tokenizer,word, model_name):\n",
    "    if model_name not in bert_cache_dict:\n",
    "        bert_cache_dict[model_name] = {}\n",
    "    if word in bert_cache_dict[model_name]:\n",
    "        return bert_cache_dict[model_name][word]\n",
    "    \n",
    "    inputs1 = tokenizer(word, return_tensors=\"pt\").to('mps')\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "    embedding = outputs1.last_hidden_state.mean(dim=1)\n",
    "    bert_cache_dict[model_name][word] = embedding\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb7426-2b4b-4282-be76-ba55e999e6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to calculate similarity\n",
    "def calculate_similarity(model, tokenizer, word1, word2, model_name):\n",
    "\n",
    "    embeddings1 = inference(model,tokenizer,word1, model_name)\n",
    "    embeddings2 = inference(model,tokenizer,word2, model_name)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = F.cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = 'bert-large-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name).to('mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762421ff-7e1f-47d2-b283-5b2764462759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "# Load a pre-trained Word2Vec model\n",
    "w2v_model = load('word2vec-google-news-300')\n",
    "    \n",
    "def calculate_similarity_w2v(model, text1, text2):\n",
    "    def get_average_vector(text):\n",
    "        vectors = [model[word] for word in text.split() if word in model.key_to_index]\n",
    "        if len(vectors) == 0:\n",
    "            return None\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    vec1 = get_average_vector(text1)\n",
    "    vec2 = get_average_vector(text2)\n",
    "\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d84d9a-a424-4f0a-b39f-d17f020c6f02",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "1. hint priority : longest word\n",
    "2. 2 x 2 pair work. We can solve two puzzles in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f8519-c425-43e3-8fa5-e06891026203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "general = [\n",
    "    widgets.Text(description=\"Topic: \",disabled=False),\n",
    "    widgets.Label('Type your general prompt here.'),\n",
    "    widgets.Text(''),\n",
    "    widgets.Label('Add your target words.'),\n",
    "    widgets.HBox([\n",
    "        widgets.Button(description='Add word'),\n",
    "        widgets.Button(description='Remove last word'),\n",
    "        widgets.Button(description='Fix word list'),\n",
    "        widgets.Button(description='check validity')\n",
    "    ]),\n",
    "]\n",
    "calc = widgets.HBox([\n",
    "    widgets.Button(description='Calculate candidates'),\n",
    "    widgets.Button(description='Explore candidates for: '),\n",
    "    widgets.IntText(description='Word id:',disabled=False),\n",
    "])\n",
    "words = widgets.Tab()\n",
    "word_values = []\n",
    "word_relations = [] # (i,p,j,q) : pth position of word i overlaps with qth position of word j\n",
    "histories = {} # set((i,word))\n",
    "\n",
    "suggestions = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(''),\n",
    "        widgets.Checkbox(value=False,label='checked?'),\n",
    "    ]) for i in range(30)\n",
    "])\n",
    "\n",
    "saving = [\n",
    "    widgets.HBox([    widgets.Button(description='Save'),\n",
    "    widgets.Button(description='Load')]),\n",
    "    widgets.HBox([\n",
    "    widgets.Label('Save/Load file name'),\n",
    "    widgets.Text('puzzle_1_backup.json')])\n",
    "]\n",
    "output = widgets.Output()\n",
    "\n",
    "def state2dict():\n",
    "    global histories\n",
    "    global general\n",
    "    global word_values\n",
    "    statedict = {'topic': general[0].value,'prompt':general[2].value, 'words':[]}\n",
    "    for word in word_values:\n",
    "        word_dict = {}\n",
    "        word_dict['len'] = word.children[0].value\n",
    "        word_dict['hint'] = word.children[1].value\n",
    "        word_dict['ans'] = word.children[-1].value\n",
    "        word_dict['rules'] = []\n",
    "        for rule in word.children[3:-1]:\n",
    "            word_dict['rules'].append([x.value for x in rule.children])\n",
    "        statedict['words'].append(word_dict)\n",
    "    statedict['histories'] = [[i,word] for i,word in histories]\n",
    "    return statedict\n",
    "\n",
    "def dict2state(statedict):\n",
    "    global histories\n",
    "    global general\n",
    "    global word_values\n",
    "    global words\n",
    "    general[0].value = statedict['topic']\n",
    "    general[2].value = statedict['prompt']\n",
    "    word_values = []\n",
    "    for word_dict in statedict['words']:\n",
    "        add_word(None)\n",
    "        word_values[-1].children[0].value = word_dict['len']\n",
    "        word_values[-1].children[1].value = word_dict['hint']\n",
    "        word_values[-1].children[-1].value = word_dict['ans']\n",
    "        for rule, rule_obj in zip(word_dict['rules'], word_values[-1].children[3:-1]):\n",
    "            for child, r in zip(rule_obj.children,rule):\n",
    "                child.value = r\n",
    "        words.children = word_values\n",
    "    histories = { (x[0],x[1]) for x in statedict['histories'] }\n",
    "    \n",
    "def progress_save(b):\n",
    "    file_name = saving[1].children[1].value\n",
    "    statedict = state2dict()\n",
    "    json.dump(statedict,open(file_name,'w'))\n",
    "    torch.save(bert_cache_dict,bert_cache_dir)\n",
    "\n",
    "def progress_load(b):\n",
    "    file_name = saving[1].children[1].value\n",
    "    statedict = json.load(open(file_name))\n",
    "    dict2state(statedict)\n",
    "\n",
    "saving[0].children[0].on_click(progress_save)\n",
    "saving[0].children[1].on_click(progress_load)\n",
    "\n",
    "def add_word(b):\n",
    "    global words\n",
    "    global word_values\n",
    "\n",
    "    # 길이, 힌트,\n",
    "    word_values.append(widgets.VBox([\n",
    "        widgets.IntText(\n",
    "                description='Length:',\n",
    "                disabled=False\n",
    "            ),\n",
    "        widgets.Text(description='Hint:',disabled=False),\n",
    "        widgets.Label('Relations. idx starts from 0. Set idx to negative to disable.'),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            widgets.IntText(value=-1,description='Char idx:',disabled=False),\n",
    "            widgets.IntText(description='Word id:',disabled=False),\n",
    "            widgets.IntText(description='Target idx:',disabled=False)\n",
    "        ]),\n",
    "        widgets.Text(description='Answer:',disabled=False)\n",
    "    ]))\n",
    "    word_values[-1].id = len(word_values)-1\n",
    "    words.children=word_values\n",
    "    words.set_title(len(word_values)-1,f\"word #{len(word_values)}\")\n",
    "    display_all()\n",
    "    \n",
    "def remove_word(b):\n",
    "    global words\n",
    "    global word_values\n",
    "    if len(word_values) > 0:\n",
    "        word_values = word_values[:-1]\n",
    "        words.children=word_values\n",
    "        display_all()\n",
    "\n",
    "fix = False\n",
    "def disable(b):\n",
    "    global general\n",
    "    global fix\n",
    "    if fix:\n",
    "        fix = False\n",
    "        general[4].children[0].disabled = False\n",
    "        general[4].children[1].disabled = False\n",
    "    else:\n",
    "        fix = True\n",
    "        general[4].children[0].disabled = True\n",
    "        general[4].children[1].disabled = True\n",
    "\n",
    "def display_all():\n",
    "    with output:\n",
    "        output.clear_output() \n",
    "        for g in general:\n",
    "            display(g)\n",
    "        if len(words.children)>0:\n",
    "            display(words)    \n",
    "        display(calc)\n",
    "        display(progress_bar)\n",
    "        for s in saving:\n",
    "            display(s)\n",
    "            \n",
    "sim_scores = defaultdict(dict) # {1:{'word':0.8,...,'hihi':0.6}}\n",
    "\n",
    "def recursive_explorer(fixed_rules,sim_scores,word_values,topic,num_per_iter = 20,skip_tuples=set()):\n",
    "    global all_word_list\n",
    "    for word in word_values:\n",
    "        i = word.id\n",
    "        if not len(word.children[-1].value) == word.children[0].value:\n",
    "            filtered_keys = []\n",
    "            for w in all_word_list:\n",
    "                if len(w) == word.children[0].value:\n",
    "                    if (i,w) in skip_tuples:\n",
    "                        continue\n",
    "                    if i in sim_scores and w in sim_scores[i]:\n",
    "                        continue\n",
    "                    chk=True\n",
    "                    for rule in fixed_rules:\n",
    "                        if rule[0] == i and w[rule[1]] != rule[2]:\n",
    "                            chk=False\n",
    "                            break\n",
    "                    if chk:\n",
    "                        filtered_keys.append(w)\n",
    "                if len(filtered_keys) >= num_per_iter:\n",
    "                    break\n",
    "            \n",
    "            for k in filtered_keys:\n",
    "                comparing_str = topic if '' == word.children[1].value else word.children[1].value\n",
    "                # if hint exist, prioritize hint.\n",
    "                sim = calculate_similarity_w2v(w2v_model, comparing_str, k)\n",
    "                # sim = calculate_similarity(model, tokenizer, comparing_str, k)\n",
    "                sim_scores[i][k] = sim * 0.8 if word.children[1].value == '' else sim\n",
    "                # give adventage to hint-driven one.\n",
    "\n",
    "def flatten_sim_scores(sim_scores):\n",
    "    ans = []\n",
    "    for i, vdict in sim_scores.items():\n",
    "        ans += [(i,word,score) for word,score in vdict.items()]\n",
    "        # ans += [(i,word,score) for word,score in vdict.items() if (i,word) not in skip_tuples]\n",
    "    ans.sort(key=lambda x:-x[2])\n",
    "    return ans\n",
    "\n",
    "def calculate_candidates(b):\n",
    "    # using only bert for this function.\n",
    "    start_time = time.time()\n",
    "    global progress_bar\n",
    "    global word_values\n",
    "    global general\n",
    "    global sim_scores\n",
    "    global suggestions\n",
    "    global histories\n",
    "    progress_bar.value = 0\n",
    "    for s in suggestions.children:\n",
    "        if s.children[1].value == True and s.children[0].value != '':\n",
    "            _ = s.children[0].value.split('\\t')\n",
    "            histories.add((int(_[0])-1,_[1]))\n",
    "            \n",
    "        \n",
    "    num_per_iter = 40\n",
    "    skip_tuples = {_ for _ in histories}\n",
    "    # first, collect fixed word\n",
    "    fixed_rules = [] # (i,p_i, x) : p_ith position of word i should be x\n",
    "    solved = set()\n",
    "    for i,word in enumerate(word_values):\n",
    "        if len(word.children[-1].value) == word.children[0].value:\n",
    "            solved.add(i)\n",
    "            for rule in word.children[3:-1]:\n",
    "                if rule.children[0].value >= 0:\n",
    "                    target = word.children[-1].value[rule.children[0].value]\n",
    "                    fixed_rules.append((rule.children[1].value-1,rule.children[2].value,target))\n",
    "    # Second. calculate top 20 candidates for each word\n",
    "    progress_bar.value = 1\n",
    "    recursive_explorer(fixed_rules,sim_scores,word_values,general[0].value,num_per_iter,skip_tuples)\n",
    "    # choose top 20 best scenarios by greedy\n",
    "    solutions = []\n",
    "    for s in solved:\n",
    "        if s in sim_scores:\n",
    "            del sim_scores[s]\n",
    "    top_scores = flatten_sim_scores(sim_scores)[:num_per_iter] # (i,word,score)\n",
    "    skip_tuples.union(set([(i,word) for i,word,sim in top_scores]))\n",
    "    for i,word,sim in top_scores:\n",
    "        progress_bar.value = progress_bar.value + 1\n",
    "        virtual_solved = {_ for _ in solved}\n",
    "        virtual_solved.add(i)\n",
    "        virtual_rules = [_ for _ in fixed_rules]\n",
    "        solution = {i:(word,sim)}\n",
    "        prev_id = i\n",
    "        prev_word = word\n",
    "        flag = True\n",
    "        while(len(virtual_solved)<len(word_values)):\n",
    "            virtual_sim_scores = defaultdict(dict)\n",
    "            for rule in word_values[prev_id].children[3:-1]:\n",
    "                if rule.children[0].value >= 0:\n",
    "                    target = prev_word[rule.children[0].value]\n",
    "                    virtual_rules.append((rule.children[1].value-1,rule.children[2].value,target))\n",
    "            recursive_explorer(\n",
    "                virtual_rules,\n",
    "                virtual_sim_scores,\n",
    "                [x for j,x in enumerate(word_values) if j not in virtual_solved],\n",
    "                general[0].value,\n",
    "                20,\n",
    "                skip_tuples\n",
    "            )\n",
    "            candis = flatten_sim_scores(virtual_sim_scores)\n",
    "            if len(candis) == 0:\n",
    "                flag = False\n",
    "                break\n",
    "            top_score = candis[0]\n",
    "            virtual_solved.add(top_score[0])\n",
    "            solution[top_score[0]] = (top_score[1],top_score[2])\n",
    "            prev_id = top_score[0]\n",
    "            prev_word = top_score[1]\n",
    "            skip_tuples.add((prev_id,prev_word))\n",
    "        if flag:\n",
    "            solutions.append(solution)\n",
    "            \n",
    "    # calculate average score for rules and display top 30.\n",
    "    solutions.sort(key=lambda x:-1*sum([v[1] for v in x.values()]))\n",
    "    s_cnt = 0\n",
    "    for solution in solutions:\n",
    "        for i, (word,sim) in solution.items():\n",
    "            suggestions.children[s_cnt].children[0].value = f\"{i+1}\\t{word}\\t{sim}\"\n",
    "            suggestions.children[s_cnt].children[1].value = False\n",
    "            s_cnt += 1\n",
    "            if s_cnt == len(suggestions.children):\n",
    "                break\n",
    "        if s_cnt == len(suggestions.children):\n",
    "            break\n",
    "    if s_cnt < len(suggestions.children):\n",
    "        for i in range(s_cnt,len(suggestions.children)):\n",
    "            suggestions.children[i].children[0].value = \"\"\n",
    "    display_all()\n",
    "    with output:\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        display(suggestions)\n",
    "        \n",
    "def explore_candidates(b): # exploring with bert.\n",
    "    global progress_bar\n",
    "    global word_values\n",
    "    global general\n",
    "    global suggestions\n",
    "    global histories\n",
    "    global calc\n",
    "    progress_bar.value = 0\n",
    "    start_time = time.time()\n",
    "    target_id = calc.children[2].value - 1\n",
    "    fixed_rules = [] # (i,p_i, x) : p_ith position of word i should be x\n",
    "    \n",
    "    for i,word in enumerate(word_values):\n",
    "        if len(word.children[-1].value) == word.children[0].value:\n",
    "            for rule in word.children[3:-1]:\n",
    "                if rule.children[0].value >= 0:\n",
    "                    target = word.children[-1].value[rule.children[0].value]\n",
    "                    if rule.children[1].value-1 == target_id:\n",
    "                        fixed_rules.append((rule.children[1].value-1,rule.children[2].value,target))\n",
    "    filtered_keys = []\n",
    "    for w in all_word_list:\n",
    "        if len(w) == word_values[target_id].children[0].value:\n",
    "            if (target_id,w) in histories:\n",
    "                continue\n",
    "            chk=True\n",
    "            for rule in fixed_rules:\n",
    "                if w[rule[1]] != rule[2]:\n",
    "                    chk=False\n",
    "                    break\n",
    "            if chk:\n",
    "                filtered_keys.append(w)\n",
    "    hint = word_values[target_id].children[1].value\n",
    "    combined_prompt = general[0].value if hint else f'{general[0].value}: {hint}'\n",
    "    word_w2v_scores = [(k,calculate_similarity_w2v(w2v_model, combined_prompt.replace(':',' '), k)) for k in filtered_keys]\n",
    "    word_w2v_scores.sort(key=lambda x:-x[1])\n",
    "    \n",
    "    top_4000 = word_w2v_scores[:4000]\n",
    "    word_bert_scores = []\n",
    "    for i,(k,_) in enumerate(top_4000):\n",
    "        if i%100 == 0:\n",
    "            progress_bar.value = progress_bar.value+1\n",
    "        word_bert_scores.append((k,calculate_similarity(bert_model,tokenizer,combined_prompt,k,model_name)) )\n",
    "    word_bert_scores.sort(key=lambda x:-x[1])\n",
    "    top_30 = word_bert_scores[:30]\n",
    "    for suggestion, (word,sim) in zip(suggestions.children,top_30):\n",
    "        suggestion.children[0].value = f\"{target_id+1}\\t{word}\\t{sim}\"\n",
    "        suggestion.children[1].value = False\n",
    "    display_all()\n",
    "    with output:\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        display(suggestions)\n",
    "\n",
    "valid_fix= False\n",
    "def validation(b):\n",
    "    global valid_fix\n",
    "    if valid_fix:\n",
    "        valid_fix=False\n",
    "        for word_value in word_values:\n",
    "            \n",
    "            word_value.children[0].disabled = False\n",
    "            for rule in word_value.children[3:-1]:\n",
    "                for ch in rule.children:\n",
    "                    ch.disabled = False\n",
    "    else:\n",
    "        is_valid = True\n",
    "        word_len = [wv.children[0].value for wv in word_values]\n",
    "        conditions = {} # {A : {B: (i,j)}}\n",
    "        for word_value in word_values:  \n",
    "            conditions[word_value.id] = {}\n",
    "            for rule in word_value.children[3:-1]:\n",
    "                if rule.children[0].value >= 0:\n",
    "                    conditions[word_value.id][rule.children[1].value - 1] = (rule.children[0].value,rule.children[2].value)\n",
    "        for a in conditions.keys():\n",
    "            for b in conditions[a].keys():\n",
    "                if a not in conditions[b]:\n",
    "                    is_valid = False\n",
    "                    with output:\n",
    "                        print(f'Not valid! word {a+1} and {b+1} have a conflicting condition')\n",
    "                        continue\n",
    "                    \n",
    "                if conditions[a][b][0] != conditions[b][a][1] or conditions[a][b][1] != conditions[b][a][0]:\n",
    "                    is_valid = False\n",
    "                    with output:\n",
    "                        print(f'Not valid! word {a+1} and {b+1} have a conflicting condition')\n",
    "                if conditions[a][b][0] >= word_len[a]:\n",
    "                    is_valid = False\n",
    "                    with output:\n",
    "                        print(f'Not valid! word {a+1} is shorter than its requirements from condition!')\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_fix= True\n",
    "            with output:\n",
    "                print('Valid!')\n",
    "            for word_value in word_values:\n",
    "                word_value.children[0].disabled = True\n",
    "                for rule in word_value.children[3:-1]:\n",
    "                    for ch in rule.children:\n",
    "                        ch.disabled = True\n",
    "    \n",
    "general[4].children[0].on_click(add_word)\n",
    "general[4].children[1].on_click(remove_word)\n",
    "general[4].children[2].on_click(disable)\n",
    "general[4].children[3].on_click(validation)\n",
    "calc.children[0].on_click(calculate_candidates)\n",
    "calc.children[1].on_click(explore_candidates)\n",
    "\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,    min=0,    max=41,    step=1,    description='Calculating:',    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "display_all()\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9732d-370b-4703-9e79-a1b57e1f16e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
